- What is Normalization and how does Normalization make training a model more stable?


- What are loss and optimizer functions and how do they work?


- What is Gradient Descent and how does it work?
Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error.

The starting point is just an arbitrary point for us to evaluate the performance. From that starting point, we will find the derivative (or slope), and from there, we can use a tangent line to observe the steepness of the slope. The slope will inform the updates to the parameters—i.e. the weights and bias. The slope at the starting point will be steeper, but as new parameters are generated, the steepness should gradually reduce until it reaches the lowest point on the curve, known as the point of convergence.

Similar to finding the line of best fit in linear regression, the goal of gradient descent is to minimize the cost function, or the error between predicted and actual y. In order to do this, it requires two data points—a direction and a learning rate. These factors determine the partial derivative calculations of future iterations, allowing it to gradually arrive at the local or global minimum (i.e. point of convergence).

- What is an activation function?
An activation function is a mathematical function used in artificial neural networks to introduce non-linearity into the output of a neuron. The activation function determines the output of a neuron given an input or set of inputs. It maps the inputs to a corresponding output in a certain range, such as 0 to 1 or -1 to 1, to represent the activation or firing of the neuron.

The choice of activation function depends on the specific problem and the type of neural network being used. The activation function helps to introduce non-linearity into the output of the neural network, allowing it to model complex relationships in the input data.

- What are the outputs of the following activation functions: ReLU, Softmax, Tanh, Sigmoid
ReLU (Rectified Linear Unit): The ReLU activation function maps all negative inputs to 0 and all positive inputs to their original value. The output range is [0, infinity).

Softmax: The softmax activation function is used for multiclass classification problems and maps a vector of K real values to a probability distribution over K classes. The function computes the exponential of each input value, normalizes the results so that they add up to 1, and outputs the normalized values as the probabilities for each class. The output range is [0, 1] and the sum of all outputs is 1.

Tanh (Hyperbolic Tangent): The tanh activation function maps inputs to a value between -1 and 1. The tanh function is similar to the sigmoid function, but it outputs values in a larger range and is symmetric around 0.

Sigmoid: The sigmoid activation function maps inputs to a value between 0 and 1. The function computes the exponential of the input and normalizes it to obtain a value in the [0, 1] range. The sigmoid function is commonly used in binary classification problems.

- What is the TPOT algorithm and how does it work?
TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.

TPOT works to search for the best machine learning pipeline for a given dataset and problem. It starts by generating a population of random pipelines, which are made up of combinations of machine learning algorithms and pre-processing steps.

TPOT then evaluates the performance of each pipeline using cross-validation and a user-specified performance metric. The pipelines with the best performance are then selected to breed the next generation of pipelines through genetic operations such as crossover and mutation.

This process continues for a specified number of generations, or until a termination criterion is met, such as a specified number of pipelines being evaluated or a certain level of performance being achieved. The best pipeline from the final generation is then selected and returned as the optimal solution.

TPOT allows the user to focus on other important aspects of the machine learning problem, such as feature engineering and model interpretability.

- What does TPOT stand for?
Tree-based Pipeline Optimization Tool