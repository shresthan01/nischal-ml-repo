- Algorithm Understanding
How does the Gradient-Boosted Tree Algorithm work in Classification? How does Gradient Boost differ from AdaBoost and Logistic Regression?


- What is a Delta Lake and how does it offer a solution to building reliable data pipelines?
Data lake is a central repository, where large sets of structured, semi-structured, and unstructured data are stored. Utilizing unstructured data for advanced analytics and machine learning, or storage of all types of data, like images, video, audio and documents, are some of the features that make data lake unique from other data solutions.

One of the measure hurdle of data pipeline is losing data due to any issue in the pipeline. Because data lake can store raw data to transformed data, we don't have to worry too much about losing the data. We can easily re-run pipeline if there is a data descrepancy because data lake contains all the raw data.

- When working with Pandas, we use the class pandas.core.frame.DataFrame and when working with the pandas API in Spark, we use the class pyspark.pandas.frame.DataFrame, are these the same, explain why or why not?

No, these two libraries anre not same. Pandas dataframe is heterogeneous two-dimensional size tabular data structured with labeled axes. It has single node and does not support parallelization. It is not distributed and hence processing in the Pandas DataFrame will be slower for a large amount of data. It follows Eager Execution, which means task is executed immediately.

Spark dataframes are distributed data collections that are organized into rows and columns. It has multiple nodes and supports parallelization. It is distributed and hence, processing in the Spark DataFrame is faster for a large amount of data. It follows Lazy Execution which means that a task is not executed until an action is performed.

- What is a Machine Learning Pipeline is and why itâ€™s important? What are the steps in a Machine Learning workflow?
Machine learning pipeline is an end-to-end construct that orchestrates the flow of data into, and output from, a machine learning model. It consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment. It includes raw data input, features, outputs, the machine learning model and model parameters, and prediction outputs.

In ML pipeline each part of the workflow is abstracted into an independent service. Then, each time when we design a new workflow, we can pick and choose which elements we need and use them where we need them. Pipeline also helps improve the model performance with continous training.
